{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9a14453",
   "metadata": {},
   "source": [
    "# Neural Network Backpropagation from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1f4756a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e23296c",
   "metadata": {},
   "source": [
    "NN is just a big function. Remember a forward pass for one neuron is basically this:\n",
    "\n",
    "$ReLU(\\sum{[inputs*weights]} + bias)$\n",
    "\n",
    "in code that is\n",
    "\n",
    "$ReLU(x_0w_0+x_1w_1+x_2w_2)$\n",
    "\n",
    "which should familar -- dot product. Weighted inputs plus bias send through an activation function -- in this case $ReLU$.\n",
    "\n",
    "One should see the need for differentiation coming up. How do the different parameters -- the $x$s and the $w$s -- influence the result? Let's solve for $x_0$. How does $ReLU()$ (params omitted for brevity) change with respect to $x_0$?\n",
    "\n",
    "There are three functions nested: $ReLU$, $sum$ and $mul$.\n",
    "\n",
    "Take note using $mul$ and $sum$ makes it a little more obvious that we're talking about functions.\n",
    "\n",
    "$x_0w_0+x_1w_1+x_2w_2=sum(mul(x_0,w_0)+mul(x_1,w_1)+mul(x_2,w_2))$\n",
    "\n",
    "We apply the **chain rule**.\n",
    "\n",
    "$\n",
    "\\begin{equation}\n",
    "    \\cfrac{\\partial}{x_0}[ReLU(sum(mul(x_0,w_0)+mul(x_1,w_1)+mul(x_2,w_2)))]\\\\\n",
    "    =\\cfrac{dReLU(sum(mul(x_0,w_0)))}{dsum(mul(x_0,w_0))}\\cdot\\cfrac{\\partial sum(mul(x_0,w_0))}{\\partial mul(x_0w_0)}\\cdot\\cfrac{\\partial mul(x_0w_0)}{\\partial w_0}\\\\\n",
    "    =ReLU(sum(mul(x_0,w_0))'(sum(mul(x_0,w_0)))\\cdot sum(mul(x_0,w_0))'(mul(x_0,w_0))\\cdot mul(x_0,w_0)'(x_0)\\\\\n",
    "\\end{equation}\n",
    "$\n",
    "\n",
    "That's it in Leibniz and Lagrange notation. We're interested in $ReLU(x_0w_0)$ as we omit the $+x_1w_1+x_2w_2$ as they don't influence $x_0$. \\\n",
    "This process gets us the derivative of the weight $w_0$ and therefore the influence of it on the greater $ReLU$ function. This is also how we know to tweak this weight for less loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16cd38f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReLU(x):\n",
    "    return max(0, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca7b39bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [1., -2., 3.]  # three inputs to the neuron\n",
    "weights = np.array([-3., -1., 2.])\n",
    "bias = 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac494e4",
   "metadata": {},
   "source": [
    "## Forward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a8ab892",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fp = ReLU(np.dot(X, weights)) + bias\n",
    "fp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f2beda",
   "metadata": {},
   "source": [
    "## Backward Pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a0535f",
   "metadata": {},
   "source": [
    "In this example we're assuming we've alread done a backward pass and that gave us a gradient of $1$ as that layer also had just one neuron.\n",
    "\n",
    "Let's do the math we discussed above for the first weight `weights[0]`.\n",
    "\n",
    "$\n",
    "\\begin{equation}\n",
    "    \\cfrac{\\partial}{x_0}[ReLU(sum(mul(x_0,w_0)+mul(x_1,w_1)+mul(x_2,w_2)))]\\\\\n",
    "    =\\cfrac{dReLU(sum(mul(x_0,w_0)))}{dsum(mul(x_0,w_0))}\\cdot\\cfrac{\\partial sum(mul(x_0,w_0))}{\\partial mul(x_0w_0)}\\cdot\\cfrac{\\partial mul(x_0w_0)}{\\partial w_0}\\\\\n",
    "    =\\cfrac{dReLU(sum(mul(1,w_0)))}{dsum(mul(1,w_0))}\\cdot\\cfrac{\\partial sum(mul(1,w_0))}{\\partial mul(1w_0)}\\cdot\\cfrac{\\partial mul(1,w_0)}{\\partial w_0}\\\\\n",
    "    =1\\cdot1\\cdot1\\\\\n",
    "    =1\n",
    "\\end{equation}\n",
    "$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec47cabb",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_w0 = 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5e795b",
   "metadata": {},
   "source": [
    "Now for `weights[1]`:\n",
    "\n",
    "$\n",
    "\\begin{equation}\n",
    "    \\cfrac{\\partial}{x_0}[ReLU(sum(mul(x_0,w_0)+mul(x_1,w_1)+mul(x_2,w_2)))]\\\\\n",
    "    =\\cfrac{dReLU(sum(mul(-2,w_1)))}{dsum(mul(-2,w_1))}\\cdot\\cfrac{\\partial sum(mul(-2,w_1))}{\\partial mul(-2,w_0)}\\cdot\\cfrac{\\partial mul(-2,w_1)}{\\partial w_1}\\\\\n",
    "    =1\\cdot1\\cdot-2\\\\\n",
    "    =-2\n",
    "\\end{equation}\n",
    "$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c2bf8b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_w1 = -2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04a3457",
   "metadata": {},
   "source": [
    "More elaborate for `weights[2]`:\n",
    "\n",
    "$\n",
    "\\begin{equation}\n",
    "    \\cfrac{\\partial}{x_0}[ReLU(sum(mul(x_0,w_0)+mul(x_1,w_1)+mul(x_2,w_2)))]\\\\\n",
    "    =\\cfrac{dReLU(sum(mul(3,w_2)))}{dsum(mul(3,w_2))}\\cdot\\cfrac{\\partial sum(mul(3,w_2))}{\\partial mul(3,w_0)}\\cdot\\cfrac{\\partial mul(3,w_2)}{\\partial w_2}\\\\\n",
    "    =\\cfrac{\\partial max(w_2, 3)}{\\partial w_2}\n",
    "     \\cdot1\\cdot\n",
    "     \\bigg(3\\cfrac{\\partial mul(w_2)}{\\partial w_2}\\bigg)\\\\\n",
    "    =1(w_2>0)\\cdot1\\cdot(3\\cdot1)\\\\\n",
    "    =1\\cdot1\\cdot(3\\cdot1)\\\\\n",
    "    =1\\cdot1\\cdot3\\\\\n",
    "    =3\n",
    "\\end{equation}\n",
    "$\n",
    "\n",
    "Pay attention to how $ReLU$ is derived and that an activation of $0$ would inevitably lead to a derivation of the backward pass of $0$. That's how neurons die.\n",
    "\n",
    "Interlude: $d$ is actually preferred over $\\partial$ when the function is univariate and therefore the number of first-order partial derivatives is 1. Albeit $\\partial$ explicitly refers to partial derivative which is what we want -- we treat all other variables as constants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c6e7a1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_w2 = 3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f4c4c6",
   "metadata": {},
   "source": [
    "The same differential operations with respect for each `X`. This is not used to optimze the weights and bias, but it's the gradient for the next backward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c0e26b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_x0 = -3.\n",
    "d_x1 = -1.\n",
    "d_x2 = 2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db5ee56",
   "metadata": {},
   "source": [
    "A gradient is a vector of partial derivatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f8b133fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([-3.0, -1.0, 2.0], array([-3., -1.,  2.]), [1.0, -2.0, 3.0])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_grad = [d_x0, d_x1, d_x2]\n",
    "w_grad = [d_w0, d_w1, d_w2]\n",
    "lr = .001  # learning rate (part of optimizer)\n",
    "X_grad, weights, w_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501ca9ca",
   "metadata": {},
   "source": [
    "At last, the updated weights would be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6f7714f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-3.001, -0.998,  1.997])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optim_w = -lr * np.array(w_grad) + weights\n",
    "optim_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "411d25f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.986000000000001"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# another forward pass shows the slight change in result\n",
    "ReLU(np.dot(X, optim_w)) + bias"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
